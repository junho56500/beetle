{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4585b010-2b71-4592-b768-35645b062773",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Lecture 9: Object Detection, Image Segmentation, Visualizing\n",
    " Vit\n",
    " \n",
    " - 구조\n",
    "    - Encoder : Multi head attention -> layer norm -> MLP -> layer norm\n",
    "    - Decoder : Masked multi head attention -> layer norm -> multi head  여기에서는 입력 x가 다시 들어감 auto regression with mask\n",
    " \n",
    "- 모델별 차이\n",
    "    - RNN : good at long sequences, not parallelizable\n",
    "    - CNN : bad for long sequences, parallelizable\n",
    "    - Transformer : great for long sequences, highly parallel but expensive O(N^2)\n",
    "\n",
    "- Vit 구조 for classification (1)\n",
    "    - N input patches 3 x 16 x 16\n",
    "    - linear projection to D dimentional vector +,\n",
    "    - add positional embedding, learned D-dim vector per position\n",
    "    - <classification> add special extra input : classification token (D dims, learned)\n",
    "    - extract same as NLP transformer\n",
    "    - output vector\n",
    "    - <classification> linear projection to C-dim vector of predicted class scores\n",
    "\n",
    "- Vit 구조 for classification (2)\n",
    "    - N input patches\n",
    "    - flatten and apply a linear transform\n",
    "    - D-dim vector per patch as the input\n",
    "    - positional encoding\n",
    "    - don't use any masking\n",
    "    - just pooling to 1 x D to predioct class scores directly\n",
    "\n",
    "- tweaking transformers\n",
    "    - layer normalization is outside the residual connection : layer norm을 self attention 보다 먼저 실행\n",
    "    - layer norm 을 RMS Norm으로 대체 -> training is a bit more stable\n",
    "    - simple MLP를 SwiGLU MLP로 대체 -> 동일 size에서 ej high dimensional non lineality를 생성 (w1,w2 -> w1,w2,w3)\n",
    "    - Mixture of Experts(MoE) -> learn Expert different MLPs, use A < E of them per token. increase params to make the model robust without increasing too much compute. we can have multiple experts in parallel\n",
    "\n",
    "\n",
    "- Semantic segmentation idea : fully convolutional\n",
    "    - semantic segmentation을 만든다고 할때 classification과 같아 한 pixel당 출력을 한다면 오랜 시간이 소모될 것임\n",
    "    - 단순 cnn만 이어서 만드는 것도 가능하겠지만 그러면 너무 많은 parameter가 소요됨\n",
    "    - 그러므로 down sampling and up sampling이 더 효율적임\n",
    "    - downsampling : pooling, strided convolution\n",
    "    - loss : softmax, regression loss, SVM loss \n",
    "    - upsampling : \n",
    "        - unpooling : nearest neighbor\n",
    "        - use position from maxpooling\n",
    "        - learnable upsampling : transposed convolution\n",
    "\n",
    "-  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
