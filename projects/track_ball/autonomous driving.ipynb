{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4585b010-2b71-4592-b768-35645b062773",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Lecture 9: Object Detection, Image Segmentation, Visualizing\n",
    " Vit\n",
    " \n",
    " - 구조\n",
    "    - Encoder : Multi head attention -> layer norm -> MLP -> layer norm\n",
    "    - Decoder : Masked multi head attention -> layer norm -> multi head  여기에서는 입력 x가 다시 들어감 auto regression with mask\n",
    " \n",
    "- 모델별 차이\n",
    "    - RNN : good at long sequences, not parallelizable\n",
    "    - CNN : bad for long sequences, parallelizable\n",
    "    - Transformer : great for long sequences, highly parallel but expensive O(N^2)\n",
    "\n",
    "- Vit 구조 for classification (1)\n",
    "    - N input patches 3 x 16 x 16\n",
    "    - linear projection to D dimentional vector +,\n",
    "    - add positional embedding, learned D-dim vector per position\n",
    "    - <classification> add special extra input : classification token (D dims, learned)\n",
    "    - extract same as NLP transformer\n",
    "    - output vector\n",
    "    - <classification> linear projection to C-dim vector of predicted class scores\n",
    "\n",
    "- Vit 구조 for classification (2)\n",
    "    - N input patches\n",
    "    - flatten and apply a linear transform\n",
    "    - D-dim vector per patch as the input\n",
    "    - positional encoding\n",
    "    - don't use any masking\n",
    "    - just pooling to 1 x D to predioct class scores directly\n",
    "\n",
    "- tweaking transformers\n",
    "    - layer normalization is outside the residual connection : layer norm을 self attention 보다 먼저 실행\n",
    "    - layer norm 을 RMS Norm으로 대체 -> training is a bit more stable\n",
    "    - simple MLP를 SwiGLU MLP로 대체 -> 동일 size에서 ej high dimensional non lineality를 생성 (w1,w2 -> w1,w2,w3)\n",
    "    - Mixture of Experts(MoE) -> learn Expert different MLPs, use A < E of them per token. increase params to make the model robust without increasing too much compute. we can have multiple experts in parallel\n",
    "\n",
    "\n",
    "- Semantic segmentation idea : fully convolutional\n",
    "    - semantic segmentation을 만든다고 할때 classification과 같아 한 pixel당 출력을 한다면 오랜 시간이 소모될 것임\n",
    "    - 단순 cnn만 이어서 만드는 것도 가능하겠지만 그러면 너무 많은 parameter가 소요됨\n",
    "    - 그러므로 down sampling and up sampling이 더 효율적임\n",
    "    - downsampling : pooling, strided convolution\n",
    "    - loss : softmax, regression loss, SVM loss \n",
    "    - upsampling :  \n",
    "        - unpooling : nearest neighbor\n",
    "        - use position from maxpooling\n",
    "        - learnable upsampling : transposed convolution\n",
    "\n",
    "-  Fast R-CNN : roi proposal method를 이용한 object detection 방법. 기존 모델 대비 하나의 conv net이 맨앞에서 유추한 후, 각 roi로 분기되어 이전에 각 roi에 conv net을 뒤에서 붙이는 방식대  더 빠르고 효율적임\n",
    "    - one big conv net for all image\n",
    "    - Region of interest from proposal method (network)\n",
    "    - crop + resize features (conv5 features)\n",
    "    - CNN (per region network)\n",
    "    - CNN으로부터 \n",
    "        1. linear + softmax -> classification \n",
    "        2. linear -> box offset\n",
    "- Region proposal network : 전체 이미지에서 rio를 찾아주는 것이 먼저 필요\n",
    "    - fixed size를 가진 anchor로 각 위치당 돌아가면서 object가 안에 있는지 먼저 찾는다 objectness score 그 중 high score를 가진 몇개를 가져와서 사용\n",
    "    - 맞는 크기로 (높은 확률을 가진 region만) coordinate을 refine\n",
    "- 현재는 사용하지 않는다. computational power가 너무 들어감 일단 네트웤이 2개로 나뉘엉 있음\n",
    "- 하나의 single stage object detector가 나옴 -> SSD, YOLO, RetinaNet \n",
    "\n",
    "- YOLO (You only look once)\n",
    "    - SxS grid로 나눔\n",
    "    - 각 grid에서 다수의 box output을 출력 \n",
    "    - box output\n",
    "        - p(object) : probablity that box가 object를 포함하는지\n",
    "        - bounding box : x,y,h,w\n",
    "        - p(class) : probability 어떤 class에 속해있는\n",
    "    - 이중에 가장 높은 probability를 가진 box로 최종 결정\n",
    "\n",
    "- DETR : detection with transformer\n",
    "    - Vit와 유사하나 object detection -> 출력이 object bounding box + classification\n",
    "    - query를 instance 개수만큼 decoder에 입력한다 여기서 query는 trainable parameter\n",
    "\n",
    "- instance segmentation\n",
    "    - Mask R-CNN\n",
    "        - R-CNN에서 add small mask network 각 roi에서 28x28 binary mask를 예측하게 한다\n",
    "        - mask는 물체의 형상을 grid map상에 존재하는 부분을 1로 표시하여 형상을 나타냄\n",
    "\n",
    "- visualization\n",
    "    - 형상 : siliency via backprop\n",
    "    - class : CAM (class activation mapping), grad CAM\n",
    "    - visualizing vit features, attention과 weight을 그려볼 수 있다\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
